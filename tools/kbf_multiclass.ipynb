{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: unsafe repository ('/MS3D' is owned by someone else)\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /MS3D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 05:48:16,053   INFO  Loading Waymo dataset\n",
      "2023-05-12 05:48:32,061   INFO  Total skipped info 0\n",
      "2023-05-12 05:48:32,062   INFO  Total samples for Waymo dataset: 37680\n",
      "2023-05-12 05:48:32,074   INFO  Total sampled samples for Waymo dataset: 18840\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/MS3D')\n",
    "from pcdet.config import cfg, cfg_from_yaml_file\n",
    "from pcdet.utils import common_utils\n",
    "from pcdet.datasets import build_dataloader\n",
    "\n",
    "cfg_file = '/MS3D/tools/cfgs/target_waymo/ms3d_nuscenes_voxel_rcnn_centerhead.yaml'\n",
    "\n",
    "# Get target dataset\n",
    "cfg_from_yaml_file(cfg_file, cfg)\n",
    "logger = common_utils.create_logger('temp.txt', rank=cfg.LOCAL_RANK)\n",
    "if cfg.get('DATA_CONFIG_TAR', False):\n",
    "    dataset_cfg = cfg.DATA_CONFIG_TAR\n",
    "    classes = cfg.DATA_CONFIG_TAR.CLASS_NAMES\n",
    "    cfg.DATA_CONFIG_TAR.USE_PSEUDO_LABEL=False\n",
    "    if dataset_cfg.get('USE_TTA', False):\n",
    "        dataset_cfg.USE_TTA=False\n",
    "else:\n",
    "    dataset_cfg = cfg.DATA_CONFIG\n",
    "    classes = cfg.CLASS_NAMES\n",
    "dataset_cfg.DATA_SPLIT.test = 'train'\n",
    "dataset_cfg.USE_CUSTOM_TRAIN_SCENES = True\n",
    "dataset_cfg.SAMPLED_INTERVAL.test = 2\n",
    "\n",
    "target_set, _, _ = build_dataloader(\n",
    "            dataset_cfg=dataset_cfg,\n",
    "            class_names=classes,\n",
    "            batch_size=1, logger=logger, training=False, dist=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9420"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['Vehicle', 'Vehicle', 'Vehicle', 'Cyclist', 'Cyclist', 'Pedestrian']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aggregating_proposals: 100%|████████████████████████████| 18840/18840 [00:30<00:00, 626.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from pcdet.utils import box_fusion_utils\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "dets_txt = '/MS3D/tools/cfgs/target_waymo/raw_dets/train5hz_dets.txt'\n",
    "print(f'classes: {cfg.DATA_CONFIG_TAR.CLASS_NAMES}')\n",
    "\n",
    "ps_dict = {}\n",
    "det_annos = box_fusion_utils.load_src_paths_txt(dets_txt)\n",
    "combined_dets = box_fusion_utils.combine_box_pkls(det_annos, cfg.DATA_CONFIG_TAR.CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:\n",
    "1. Have diff discard/radius for diff classes\n",
    "Maybe something like this?\n",
    "```yaml\n",
    "# Check that all lists are same length\n",
    "classes: ['Vehicle', 'Vehicle', 'Vehicle', 'Cyclist', 'Cyclist', 'Pedestrian']\n",
    "discard: [4,4,4, 4,4, 4]\n",
    "radius: [2,2,2, 0.5,0.5, 0.3]\n",
    "```\n",
    "But if overlapping classes e.g. cyclist and pedestrian, you'd want the predominant class to be kept. Maybe it's okay to just match by class, then later on use NMS to keep the more confident classes?\n",
    "\n",
    "2. Weighing by detection quantity\n",
    "\n",
    "I wonder if there's some way to weigh by the number of detections it received? Low score but many detections make for a strong proposal.\n",
    "\n",
    "#### Comments\n",
    "Fine-tuning lyft -> waymo for ITSC submission was fine to use car,truck,bus -> vehicle,vehicle,vehicle. So I guess it doesn't affect the training to map it like this. I think it probably rendered the truck/bus heads useless since we labelled everything as 1 i.e. car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_scaling_coeff(num_boxes, num_boxes_at_unity, eq_type='sqrt', max_scaling=2.0):\n",
    "    \"\"\"\n",
    "    This function returns the scaling coefficient to scale the confidence score based on\n",
    "    the number of boxes proposed for an object. Two scaling equations are provided.\n",
    "    \n",
    "    Note: This might be bad if many detectors detect but wrongly detect -> adjust max_scaling according\n",
    "    \n",
    "    num_boxes (list of ints): number of input boxes for each kbf box fusion\n",
    "    num_boxes_at_unity (int): The point at which the scaling coeff is equal to 1. \n",
    "                        Less than this and coeff < 1; More than this and coeff > 1.\n",
    "    \"\"\"\n",
    "    if eq_type == 'sqrt':\n",
    "        # Non-linear scaling with sqrt(x)        \n",
    "        scaling = (1/np.sqrt(num_boxes_at_unity)) * np.sqrt(num_boxes) \n",
    "        \n",
    "    elif eq_type == 'linear':\n",
    "        scaling = (1/num_boxes_at_unity) * num_boxes\n",
    "        \n",
    "    else:\n",
    "        raise notImplementedError\n",
    "        \n",
    "    return np.clip(scaling, a_max=max_scaling, a_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 3768/3768 [09:54<00:00,  6.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# ['Vehicle', 'Vehicle', 'Vehicle', 'Cyclist', 'Cyclist', 'Pedestrian']\n",
    "classes=target_set.dataset_cfg.CLASS_NAMES\n",
    "discard=[2,2,2,2,2,2]\n",
    "radius=[2,2,2, 0.5,0.5, 0.3]\n",
    "kbf_nms=[0.1,0.1,0.1,0.5,0.5,0.5,0.5]\n",
    "min_score = 0.3\n",
    "\n",
    "# confidence scaling\n",
    "scale_conf_by_ndets = False\n",
    "num_boxes_at_unity = 3\n",
    "max_scale_score = cfg.SELF_TRAIN.SCORE_THRESH + 0.1\n",
    "\n",
    "# Downsample\n",
    "ds_combined_dets = combined_dets[::5]\n",
    "\n",
    "# Get class specific config\n",
    "cls_kbf_config = {}\n",
    "for enum, cls in enumerate(classes):\n",
    "    if cls in cls_kbf_config.keys():\n",
    "        continue\n",
    "    cls_kbf_config[cls] = {}\n",
    "    cls_kbf_config[cls]['cls_id'] = enum+1 # in OpenPCDet, cls_ids enumerate from 1\n",
    "    cls_kbf_config[cls]['discard'] = discard[enum]\n",
    "    cls_kbf_config[cls]['radius'] = radius[enum]\n",
    "    cls_kbf_config[cls]['nms'] = kbf_nms[enum]\n",
    "\n",
    "for frame_boxes in tqdm(ds_combined_dets, total=len(ds_combined_dets)):\n",
    "        \n",
    "    boxes_lidar = np.hstack([frame_boxes['boxes_lidar'],\n",
    "                             frame_boxes['class_ids'][...,np.newaxis],\n",
    "                             frame_boxes['score'][...,np.newaxis]])\n",
    "    \n",
    "    boxes_names = frame_boxes['names']\n",
    "    unique_classes = np.unique(boxes_names)    \n",
    "    ps_label_nms = []\n",
    "    for cls in unique_classes:\n",
    "        cls_mask = (boxes_names == cls)\n",
    "        cls_boxes = boxes_lidar[cls_mask]\n",
    "        score_mask = cls_boxes[:,8] > min_score\n",
    "        cls_kbf_boxes, num_dets_per_box = box_fusion_utils.label_fusion(cls_boxes, 'kde_fusion', \n",
    "                                       discard=cls_kbf_config[cls]['discard'], \n",
    "                                       radius=cls_kbf_config[cls]['radius'], \n",
    "                                       nms_thresh=cls_kbf_config[cls]['nms'], \n",
    "                                       weights=None)\n",
    "        \n",
    "        # E.g. car,truck,bus,motorcycle,bicycle,pedestrian (id: 1,2,3,4,5,6) -> Veh,Cyc,Ped (id: 1,4,6)\n",
    "        cls_kbf_boxes[:,7] = cls_kbf_config[cls]['cls_id']\n",
    "        \n",
    "        # Scale confidence score by number of detections\n",
    "        if scale_conf_by_ndets:\n",
    "            coeff = conf_scaling_coeff(num_dets_per_box, num_boxes_at_unity)\n",
    "            mask = cls_kbf_boxes[:,8] < cfg.SELF_TRAIN.SCORE_THRESH\n",
    "            cls_kbf_boxes[mask,8] *= coeff[mask]\n",
    "            cls_kbf_boxes[mask,8]= np.clip(cls_kbf_boxes[mask,8], 0, max_scale_score)\n",
    "            \n",
    "        ps_label_nms.extend(cls_kbf_boxes)\n",
    "        \n",
    "    ps_label_nms = np.array(ps_label_nms)\n",
    "    \n",
    "    # neg_th < score < pos_th: ignore for training but keep for update step\n",
    "    pred_boxes = ps_label_nms[:,:7]\n",
    "    pred_labels = ps_label_nms[:,7]\n",
    "    pred_scores = ps_label_nms[:,8]\n",
    "    ignore_mask = pred_scores < cfg.SELF_TRAIN.SCORE_THRESH\n",
    "    pred_labels[ignore_mask] = -pred_labels[ignore_mask]\n",
    "    gt_box = np.concatenate((pred_boxes,\n",
    "                            pred_labels.reshape(-1, 1),\n",
    "                            pred_scores.reshape(-1, 1)), axis=1)    \n",
    "\n",
    "    gt_infos = {\n",
    "        'gt_boxes': gt_box,\n",
    "    }\n",
    "    ps_dict[frame_boxes['frame_id']] = gt_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 4220/4220 [00:00<00:00, 17627.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = target_set\n",
    "\n",
    "pred_annos, gt_annos = [], []\n",
    "eval_gt_annos = copy.deepcopy(dataset.infos)\n",
    "for frame_id in tqdm(ps_dict.keys(), total=len(ps_dict.keys())):  \n",
    "    boxes = ps_dict[frame_id]['gt_boxes'].copy()            \n",
    "    boxes[:,:3] -= dataset.dataset_cfg.SHIFT_COOR # just for eval in this notebook\n",
    "    p_anno = {\"frame_id\": frame_id,\n",
    "              \"name\": np.array([dataset.dataset_cfg.CLASS_NAMES[int(abs(box[7]))-1] for box in boxes]),\n",
    "              \"pred_labels\": np.ones(len(boxes)),\n",
    "              \"boxes_lidar\": boxes[:,:7],\n",
    "              \"score\": boxes[:,8]}\n",
    "    pred_annos.append(p_anno)\n",
    "    gt_annos.append(eval_gt_annos[dataset.frameid_to_idx[frame_id]]['annos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the waymo evaluation...\n",
      "Number: (pd, 244696) VS. (gt, 353781)\n",
      "Level 1: 293563, Level2: 60218\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/contextlib.py:82: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `self.session()` or `self.cached_session()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 07:03:58.780081: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-05-12 07:03:58.782075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-12 07:03:58.782879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 computeCapability: 7.5\n",
      "coreClock: 1.2GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2023-05-12 07:03:58.783758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.7/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-12 07:03:58.783961: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.7/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-12 07:03:58.784218: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.7/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-12 07:03:58.796844: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-05-12 07:03:58.799807: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-05-12 07:03:58.800070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.7/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-12 07:03:58.800404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.7/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-12 07:03:58.800533: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.7/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-05-12 07:03:58.800547: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-12 07:03:58.802453: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 07:03:58.803302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-05-12 07:03:58.803319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2023-05-12 07:03:58.817070: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2599990000 Hz\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0512 07:03:58.906677   511 detection_metrics_ops.cc:157] Computing detection metrics for 244696 predicted boxes.\n",
      "I0512 07:03:58.906718   511 detection_metrics_ops.cc:159] Parsing prediction [244696,7][244696]\n",
      "I0512 07:03:59.023898   511 detection_metrics_ops.cc:168] Parsing ground truth [353781,7][353781]\n"
     ]
    }
   ],
   "source": [
    "# ap_result_str, ap_dict = dataset.kitti_eval(pred_annos, eval_gt_annos, class_names=['Vehicle'])\n",
    "# ap_result_str, ap_dict = dataset.evaluation(pred_annos, dataset.dataset_cfg.CLASS_NAMES, eval_metric='waymo')\n",
    "from pcdet.datasets.waymo.waymo_eval import OpenPCDetWaymoDetectionMetricsEstimator\n",
    "eval = OpenPCDetWaymoDetectionMetricsEstimator()\n",
    "\n",
    "ap_dict = eval.waymo_evaluation(\n",
    "    pred_annos, gt_annos, class_name=dataset.dataset_cfg.CLASS_NAMES,\n",
    "    distance_thresh=1000, fake_gt_infos=dataset.dataset_cfg.get('INFO_WITH_FAKELIDAR', False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling by number of detectors\n",
    "\n",
    "This desmos grapher was useful for coming up with below https://www.desmos.com/calculator\n",
    "\n",
    "At the end of the day we just wanted to map one range to another in a non-linear fashion where we can specify how many detections would give a scaling=1; or what the max scaling is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8164965809277261, 1.4142135623730951)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a config where you can enable/disable this\n",
    "def min_boxes_conf_scaling(num_boxes, min_boxes, eq_type='sqrt', max_scaling=2.0):\n",
    "    \"\"\"\n",
    "    For this function, at min_boxes, scaling is 1. We clip if it gets too large.    \n",
    "    Adjustable param: min_boxes\n",
    "    \"\"\"\n",
    "    if eq_type == 'sqrt':\n",
    "        # Non-linear scaling with sqrt(x)        \n",
    "        scaling = (1/np.sqrt(min_boxes)) * np.sqrt(num_boxes) \n",
    "        \n",
    "    elif eq_type == 'linear':\n",
    "        scaling = (1/min_boxes) * num_boxes\n",
    "        \n",
    "    else:\n",
    "        raise notImplementedError\n",
    "        \n",
    "    return np.clip(scaling, a_max=max_scaling, a_min=0)\n",
    "\n",
    "\n",
    "def total_dets_conf_scaling(num_boxes, total_num_detectors, eq_type='sqrt', max_scaling=2.0):\n",
    "    \"\"\"\n",
    "    For this function, at total_num_detectors, scaling is max_scaling. The point where scaling=1\n",
    "    will change according to the total_num_detectors which may not be optimal.\n",
    "    \n",
    "    Adjustable param: total_num_detectors\n",
    "    \"\"\"\n",
    "    if eq_type == 'sqrt':\n",
    "        # Non-linear scaling with sqrt(x)        \n",
    "        scaling = (max_scaling/np.sqrt(total_num_detectors)) * np.sqrt(num_boxes) \n",
    "        \n",
    "    elif eq_type == 'linear':\n",
    "        scaling = (max_scaling/total_num_detectors) * num_boxes\n",
    "        \n",
    "    else:\n",
    "        raise notImplementedError\n",
    "        \n",
    "    return np.clip(scaling, a_max=max_scaling, a_min=0)\n",
    "\n",
    "\n",
    "# min_dets = 20\n",
    "# x = np.arange(0,30)\n",
    "# # y = (1/np.sqrt(min_dets)) * np.sqrt(x) \n",
    "# y = 1/20 * x\n",
    "# plt.plot(x,y)\n",
    "# plt.grid(True)\n",
    "\n",
    "# Non-linear scaling with sqrt(x)\n",
    "# using 2/sqrt(num_det) means that at num_dets, scaling is max_scaling -> But we don't set min_dets\n",
    "# Adjustable param: max_scaling\n",
    "# num_detectors = 24\n",
    "# max_scaling = 2\n",
    "# x = np.arange(0,30)\n",
    "# y = (max_scaling/np.sqrt(num_detectors)) * np.sqrt(x) \n",
    "# plt.plot(x,y)\n",
    "# plt.grid(True)\n",
    "scale1 = total_dets_conf_scaling(1, 6, eq_type='sqrt') # if total detectors is low, then the scaling is jumpy\n",
    "scale2 = min_boxes_conf_scaling(6, 3, eq_type='sqrt') # works better for varying min_boxes\n",
    "scale1, scale2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
